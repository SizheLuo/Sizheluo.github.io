---
layout: post
title: "The Google File System论文"
date: 2023-03-23
description: "The Google File System论文"
tag: 分布式系统
---

### 前言

回顾这篇2003年的论文，GFS可以说是“技术上辉煌而工程上保守”。说GFS技术上辉煌，是因为Google通过廉价的PC级别的硬件，搭建出了可以处理整个互联网网页数据的系统。而说GFS工程上保守，则是因为GFS没有“发明”什么特别的黑科技，而是在工程上做了大量的取舍（trade-off）。

### 目录

* [GFS的设计决策](#chapter1)
* [GFS 系统架构与设计](#chapter2)
* [GFS 读流程](#chapter3)
* [GFS 写流程](#chapter4)
* [GFS Consistency Model（一致性模型）](#chapter5)
* [总结](#chapter6)

### <a name="chapter1"></a> 1. GFS 的设计决策

![](https://s3.uuu.ovh/imgs/2023/04/08/26bdf65138002af6.png)

##### <font color="#0000dd">三个原则</font><br /> #####
##### 1. 以工程上“简单”为设计原则  #####
GFS直接使用了Linux服务上的`(1)普通文件系统作为基础存储层`，并且选择了最简单的`(2)单Master`设计。单Master让GFS的架构变得非常简单，避免了需要管理复杂的一致性问题。不过它也带来了很多限制，比如一旦Master出现故障，整个集群就无法写入数据，所以GFS其实算不上一个`高可用`的系统。

但另外一方面，GFS还是采用了`Checkpoints`、`操作日志（Operation Logs）`、`影子Master（Shadow Master）`等一系列的工程手段，来尽可能地保障整个系统的`可恢复（Recoverable）`，以及读层面的`可用性（Availability）`。

##### 2. 根据硬件特性来进行设计取舍  #####
2003年，大家都还在用机械硬盘，随机读写的性能很差，所以在GFS的设计中，重视的是顺序读写的性能，对随机写入的一致性甚至没有任何保障。

##### 3. 根据实际应用的特性，放宽了数据一致性（consistency）的选择  #####
论文里也提到，GFS是为了在廉价硬件上进行大规模数据处理而设计的。所以GFS的一致性相当宽松。GFS本身对于`随机写入的一致性没有任何保障`，而是把这个任务交给了客户端。对于`追加写入（Append）`，GFS 也只是作出了`至少一次（At LeastOnce）`这样宽松的保障。

### <a name="chapter2"></a>2. GFS 系统架构与设计

**2.1 GFS 集群的系统架构与设计**

一个 GFS cluster（集群）分为两个组件：

(1). `单个`master节点；

(2). `多个`chunkserver节点；

一个GFS集群同时可以被多个client（客户）节点访问。

一个 GFS 集群的架构可以用下图表示：

![](https://s3.uuu.ovh/imgs/2023/04/02/9d1f08752a4ee206.png)

可见GFS集群是一个典型Master + Worker结构。

	Master + Worker 结构说的是存在一个 Master 来管理任务、分配任务，而 Worker 是真正干活的节点。在这里干的活自然是数据的存储和读取。

**2.2 分块、三副本机制**

![](https://s3.uuu.ovh/imgs/2023/04/03/651312aedb3a6d14.png)

为什么要分块？这是分布式系统在各个工业领域广泛应用的原因：`并行操作`。

GFS 以 64 MB 为固定的 Chunk Size 大小。大的 chunk 有如下的`优点`：

（1）减少 Client 与 Master 服务器交互的次数，因为对同一块进行多次读写仅仅需要向 Master 服务器发出一次初始请求`（客户端缓存机制）`。这可以有效地减少 Master 的工作负载；

（2）减少了 GFS Client 与 GFS chunkserver 进行交互的数据开销，这是因为数据的读取具有连续读取的倾向，即读到 offset 的字节数据后，下一次读取有较大的概率读紧挨着 offset 数据的后续数据，chunk 的大尺寸相当于提供了一层缓存，减少了网络 I/O 的开销；

（3）减少了存储在主服务器上的元数据的大小，有利于实现元数据全缓存。

`缺点`：小数据量（比如仅仅占据一个 chunk 的文件，文件至少占据一个 chunk）的文件很多时，当很多 GFS Client 同时将 record 存储到该文件时就会造成局部的 hot spots 热点。

**2.3 单master节点设计**

Master其实有三种不同的身份，分别是：

- 相对于存储数据的`Chunkserver`，Master是一个目录服务；
- 相对于为了灾难恢复的`Backup Master`，它是一个`同步复制的主从架构`下的主节点；
- 相对于为了保障读数据的可用性而设立的`Shadow Master`，它是一个`异步复制的主从架构`下的主节点。

并且，这三种身份是依靠不同的独立模块完成的，互相之间并不干扰。本节的内容主要介绍常见的`主从架构（Master-Slave）`下的Master的职责，以及数据复制的模式。

**（1）Master 的第一个身份：一个目录服务**

master 里面会存放三种主要的元数据（metadata）：

- 文件和chunk 的命名空间信息，也就是类似前面`/data/geektime/bigdata/gfs01`这样的路径和文件名；
- 这些文件被拆分成了哪几个chunk，也就是这个全路径文件名到多个`chunk handle`的映射关系；
- 这些 chunk 实际被存储在了哪些 chunkserver 上，也就是 chunk handle 到 chunkserver 的映射关系。

![](https://s3.uuu.ovh/imgs/2023/04/02/13f44c34eff63650.png)

三者之间的KV组合关系为：

	Table 1：
		· key：
			file name
		· value：
			an array of chunk handler (nv)
	Table 2：
		· key：
			chunk handler
		· value：
			a list of chunkserver(v)
			chunk version number(nv)
			which chunkserver is primary node(which means others are nomal chunkserver in the list)(v)
			lease expiration time(v)

在 GFS 系统中，每一个 chunk 的唯一识别符是 chunk handler （有些地方也叫chunk index）。

Table1 的具体数据结构基于 HashMap 比较好，Table2 的具体实现通过 B+Tree 比较好。因为前者是文件名称作为 key，这是 HashMap 擅长的，查找的效率在 O(1)。而客户端可能一次性涉及读取大字节的数据，因此可能需要一次读取多个 chunk 的数据，涉及多个 chank handler， 这意味着最终要根据范围进行查找，范围查找 HashMap 并不擅长，B+Tree 则更擅长。

GFS 系统中，为了加快响应客户端关于 metadata 数据的请求，因此会将 metadata 存储于内存中，但是因为内存是易失性存储体，因此还需要持久化操作。具体来说：

客户端向 Master 节点请求的 metadata 数据直接存储于 Master 的内存中，避免每次请求都需要进行磁盘 I/O；

Master 节点使用日志 + checkpoint 的方式来确保数据的持久化；

上面表的介绍中括号中的 nv 含义是 Non-Volatile，也就是非易失性的含义，即要求将数据存储到磁盘上，而 v 的含义便是 Volatile，这些数据不需要持久化，当 Master 节点重启时，通过和每一个 chunkserver 进行通信来初始化不需要持久化的数据。

Master 节点内存中的 Table1 与 Table2 中的 nv 数据会定期存储到 Master 的磁盘上（包括 shadow Master 节点）；

**（2）Master 的第二个身份：Backup Master**

在单master的设计下，master 节点的所有数据，都是保存在内存里的。这样 master 的性能才能跟得上几百个客户端的并发访问。

但是数据放在内存里带来的问题，就是一旦 master 挂掉，数据就会都丢了。所以，master 会通过记录操作日志和定期生成对应的 Checkpoints 进行持久化，也就是写到硬盘上。

这是为了确保在 master 里的这些数据，不会因为一次机器故障就丢失掉。当 master 节点重启的时候，就会先读取最新的 Checkpoints，然后重放（replay）Checkpoints 之后的操作日志，把 master 节点的状态恢复到之前最新的状态。这是最常见的存储系统会用到的可恢复机制。

当然，光有这些还不够，如果只是 master 节点重新启动一下，从 Checkpoints 和日志中恢复到最新状态自然是很快的。可要是 master 节点的硬件彻底故障了呢？

所以 GFS 还为 master 准备好了几个“备胎”，也就是另外几台 Backup Master。所有针对 master 的数据操作，都需要同样写到另外准备的这几台服务器上。只有当数据在 master 上操作成功，对应的操作记录刷新到硬盘上，并且这几个 Backup Master 的数据也写入成功，并把操作记录刷新到硬盘上，整个操作才会被视为操作成功。

这种方式，叫做数据的`同步复制`，是分布式数据系统里的一种典型模式。假如你需要一个高可用的 MySQL 集群，一样也可以采用同步复制的方式，在主从服务器之间同步数据。

![](https://s3.uuu.ovh/imgs/2023/04/03/9cd0f59f74243b96.png)

而在同步复制这个机制之外，在集群外部还有监控 master 的服务在运行。如果只是 master 的进程挂掉了，那么这个监控程序会立刻重启 master 进程。而如果 master 所在的硬件或者硬盘出现损坏，那么这个监控程序就会在前面说的 Backup Master 里面找一个出来，启动对应的 master 进程，让它“备胎转正”，变成新的 master。而这个里面的数据，和原来的 master 其实一模一样。

不过，为了让集群中的其他 chunkserver 以及客户端不用感知这个变化，GFS 通过一个规范名称（Canonical Name）来指定 master，而不是通过 IP 地址或者 Mac 地址。这样，一旦要切换 master，这个监控程序只需要修改 DNS 的别名，就能达到目的。有了这个机制，GFS 的 master 就从之前的可恢复（Recoverable），进化成了能够快速恢复（Fast Recovery）。

不过，就算做到了快速恢复，我们还是不满足。毕竟，从监控程序发现 master 节点故障、启动备份节点上的 master 进程、读取 Checkpoints 和操作日志，仍然是一个几秒级别乃至分钟级别的过程。在这个时间段里，我们可能仍然有几百个客户端程序“嗷嗷待哺”，希望能够在 GFS 上读写数据。虽然作为单个 master 的设计，这个时候的确是没有办法去写入数据的。

**（3）Master 的第三个身份：Shadow Master**

为了让我们这个时候还能够从 GFS 上读取数据。加入一系列只读的`影子 Master`，这些影子 Master 和前面的备胎不同，master 写入数据并不需要等到影子 Master 也写入完成才返回成功。而是影子 Master 不断同步 master 输入的写入，尽可能保持追上 master 的最新状态。

这种方式，叫做数据的`异步复制`，是分布式系统里另一种典型模式。异步复制下，影子 Master 并不是和 master 的数据完全同步的，而是可能会有一些小小的延时。

影子 Master 会不断同步 master 里的数据，不过当 master 出现问题的时候，客户端们就可以从这些影子 Master 里找到自己想要的信息。当然，因为小小的延时，客户端有很小的概率，会读到一些过时的 master 里面的信息，比如命名空间、文件名等这些元数据。

但你也要知道，这种情况其实只会发生在以下三个条件都满足的情况下：

第一个，是 master 挂掉了；

第二个，是挂掉的 master 或者 Backup Master 上的 Checkpoints 和操作日志，还没有被影子 Master 同步完；

第三个，则是我们要读取的内容，恰恰是在没有同步完的那部分操作上；

![](https://s3.uuu.ovh/imgs/2023/04/03/95e4b5c963b80976.png)

相比于这个小小的可能性，影子 Master 让整个 GFS 在 master 快速恢复的过程中，虽然不能写数据，但仍然是完全可读的。至少在集群的读取操作上，GFS 可以算得上是`高可用（High Availability）`的了。

### <a name="chapter3"></a>3. GFS 读流程

![](https://s3.uuu.ovh/imgs/2023/04/02/9d1f08752a4ee206.png)


（1）客户端先去问 master，我们想要读取的数据在哪里。这里，客户端会发出两部分信息，一个是文件名，另一个则是要读取哪一段数据，也就是读取文件的 offset 及 length。因为所有文件都被切成 64MB 大小的一个 chunk 了，所以根据 offset 和 length，我们可以很容易地算出客户端要读取的数据在哪几个 chunk 里面。于是，客户端就会告诉 master，我要哪个文件的第几个 chunk。

（2）master 拿到了这个请求之后，就会把这个 chunk 对应的所有副本所在的 

（3）chunkserver，告诉客户端。

（4）等客户端拿到 chunk 所在的 chunkserver 信息后，客户端就可以直接去找其中任意的一个 chunkserver 读取自己所要的数据。

### <a name="chapter4"></a>4. GFS 写流程

**4.1 GFS数据写入流程**

![](https://s3.uuu.ovh/imgs/2023/04/03/f10d693cfa669788.png)

第一步，客户端会去问 master 要写入的数据，应该在哪些 chunkserver 上。

第二步，和读数据一样，master 会告诉客户端所有的次副本（secondary replica）所在的 chunkserver。这还不够，master 还会告诉客户端哪个 replica 是“老大”，也就是主副本（primary replica），数据此时以它为准。

第三步，拿到数据应该写到哪些 chunkserver 里之后，客户端会把要写的数据发给所有的 replica。不过此时，chunkserver 拿到发过来的数据后还不会真的写下来，只会把数据放在一个 LRU 的缓冲区里。

第四步，等到所有次副本都接收完数据后，客户端就会发送一个写请求给到主副本。我在上节课一开始就说过，GFS 面对的是几百个并发的客户端，所以主副本可能会收到很多个客户端的写入请求。主副本自己会给这些请求排一个顺序，确保所有的数据写入是有一个固定顺序的。接下来，主副本就开始按照这个顺序，把刚才 LRU 的缓冲区里的数据写到实际的 chunk 里去。

第五步，主副本会把对应的写请求转发给所有的次副本，所有次副本会和主副本以同样的数据写入顺序，把数据写入到硬盘上。

第六步，次副本的数据写入完成之后，会回复主副本，我也把数据和你一样写完了。

第七步，主副本再去告诉客户端，这个数据写入成功了。而如果在任何一个副本写入数据的过程中出错了，这个出错都会告诉客户端，也就意味着这次写入其实失败了。

所以在 GFS 的数据写入过程中，可能会出现主副本写入成功，但是次副本写入出错的情况。在这种情况下，客户端会认为写入失败了。但是这个时候，同一个 chunk 在不同chunkserver 上的数据可能会出现不一致的情况

**4.2 GFS数控分离**

GFS 客户端只从 master 拿到了 chunk data 在哪个 chunkserver 的元数据，实际的数据读写都不再需要通过 master。另外，不仅具体的数据传输不经过 master，后续的数据在多个 chunkserver 上同时写入的协调工作，也不需要经过 master。

这也就是说，控制流和数据流的分离，不仅仅是数据写入不需要通过 master，更重要的是实际的数据传输过程，和提供写入指令的动作是完全分离的。

**4.3 流水线（pipeline）式的网络传输**

Data Flow 传输模型的两个关键字：`linearly(线型)`、`pipeline (管道)`。

![](https://s3.uuu.ovh/imgs/2023/04/03/e27107c7ead1374c.png)

GFS 是采用了流水线（pipeline）式的网络传输。数据不一定是先给到主副本，而是看网络上离哪个 chunkserver 近，就给哪个 chunkserver，数据会先在 chunkserver 的缓冲区里存起来，就是前面提到的第 3 步。但是写入操作的指令，也就是上面的第 4~7 步，则都是由客户端发送给主副本，再由主副本统一协调写入顺序、拿到操作结果，再给到客户端的。

**4.4 独特的 Snapshot 操作**

那么，在做了分离控制流和数据流，以及使用流水线式的数据传输方式之后，对于 GFS 的网络传输上，GFS 为常见的文件复制操作单独设计一个指令。

复制文件，相信这个是你用自己的电脑的时候，会常常做的事儿。在 GFS 上，如果我们用笨一点的办法，自然是通过客户端把文件从 chunkserver 读回来，再通过客户端把数据写回去。这样的话，读数据也经过一次网络传输，写回三个副本服务器，即使是流水线式的传输，也要三次传输，一共需要把数据在网络上搬运四次。

所以，GFS 就专门为文件复制设计了一个 Snapshot 指令，当客户端通过这个指令进行文件复制的时候，这个指令会通过控制流，下达到主副本服务器，主副本服务器再把这个指令下达到次副本服务器。不过接下来，客户端并不需要去读取或者写入数据，而是各个 chunkserver 会直接在本地把对应的 chunk 复制一份。这样，数据流就完全不需要通过网络传输了。

### <a name="chapter5"></a>5. GFS Consistency Model（一致性模型）

**5.1 GFS 系统中对 file region 状态的概念定义**

在 GFS 里面，主要定义了对一致性的两个层级的概念：

第一个，就叫做“一致的（Consistent）”。这个就是指，多个客户端无论是从主副本读取数据，还是从次副本读取数据，读到的数据都是一样的。

第二个，叫做“确定的（Defined）”。这个要求会高一些，指的是对于客户端写入到GFS 的数据，能够完整地被读到。

![](https://s3.uuu.ovh/imgs/2023/04/03/4b44260e511a49a9.png)

**首先，如果数据写入失败，GFS 里的数据就是不一致的。**

这个很容易理解，GFS 里面的数据写入，并不是一个事务。上一讲里说过，主副本会把写入指令下发到两个次副本，如果次副本写入失败了，它会告诉主副本。但是，此时主副本和另一个次副本都已经写入成功了。那么这个时候，GFS 里的三个副本的数据，就是不一致的了。不同的客户端，就可能读到不同的数据。

**其次，如果客户端的数据写入是顺序的，并且写入成功了，那么文件里面的内容就是确定的。**

比如，你先往一个文件里，写入一部电影《星球大战》，这个时候，客户端无论从哪个副本读数据，读到的都是星球大战。然后再写入《星际迷航》，那么客户端再读数据，读到的也一定是《星际迷航》。

**但是，如果由多个客户端并发写入数据，即使写入成功了，GFS 里的数据也可能会进入一个一致但是非确定的状态。**

也就是说，两个客户端并发往一个文件里面写数据，一个想要写入《星球大战》，一个想要写入《星际迷航》，两个写入都成功了。这个时候，GFS 里面三份副本的数据是一样的，客户端读到的数据无论是从哪个副本里读，都是一样的。

但是呢，客户端可能读出来的数据里，前一小时是《星球大战》，后一小时是《星际迷航》。无论哪个时间节点去读数据，客户端都不能读到一部完整的《星球大战》，或者是《星际迷航》。

![](https://s3.uuu.ovh/imgs/2023/04/03/629ca878e7e81780.png)

**为什么 GFS 的数据写入会出现一致但是非确定的状态呢？**

第一种因素是在 GFS 的数据读写中，为了减轻 Master 的负载，数据的写入顺序并不需要通过 Master 来进行协调，而是直接由存储了主副本的 chunkserver，来管理同一个chunk 下数据写入的操作顺序。


第二种因素是随机的数据写入极有可能要跨越多个 chunk。

我们在写入《星球大战》和《星际迷航》的时候，前一个小时的电影是在 chunk 1，对应的主副本在 server A，后一个小时的电影是在 chunk 2，对应的主副本在 server B。然后写入请求到 server A 的时候，《星际迷航》在前，《星球大战》在后，那么《星球大战》的数据就覆盖了《星际迷航》。

而到 server B 的时候则是反过来，《星际迷航》又覆盖了《星球大战》。于是，就会出现客户端读数据，前半段是《星球大战》，后半段是《星际迷航》的奇怪现象了。


其实，这个一致但是非确定的状态，是因为随机的数据写入，没有`原子性（Atomic）`或者`事务性（Transactional）`。如果想要随机修改 GFS 上的数据，一般会建议使用方在客户端的应用层面，保障数据写入是顺序的，从而可以避免并发写入的出现。

**5.2 Record Appends 操作"至少一次（At LeastOnce）"的保障**

GFS 设计了一个专门的操作，叫做`记录追加（Record Appends）`。这是 GFS 希望我们主要使用的数据写入的方式，而且它是原子性（Atomic）的。

![](https://s3.uuu.ovh/imgs/2023/04/03/e9590641ebb933e8.png)

GFS 的记录追加和普通写之间的差别主要在于，GFS 并不会指定在 chunk 的哪个位置上写入数据，而是告诉最后一个 chunk 所在的主副本服务器，“我”要进行记录追加。

这个时候，主副本所在的 chunkserver 会做这样几件事情：

1、检查当前的 chunk 是不是可以写得下现在要追加的记录。如果写得下，那么就把当前的追加记录写进去，同时，这个数据写入也会发送给其他次副本，在次副本上也写一遍。

2、如果当前 chunk 已经放不下了，那么它先会把当前 chunk 填满空数据，并且让次副本也一样填满空数据。然后，主副本会告诉客户端，让它在下一个 chunk 上重新试验。这时候，客户端就会去一个新的 chunk 所在的 chunkserver 进行记录追加。

3、因为主副本所在的 chunkserver 控制了数据写入的操作顺序，并且数据只会往后追加，所以即使在有并发写入的情况下，请求也都会到主副本所在的同一个 chunkserver 上排队，也就不会有数据写入到同一块区域，覆盖掉已经被追加写入的数据的情况了。

4、而为了保障 chunk 里能存的下需要追加的数据，GFS 限制了一次记录追加的数据量是16MB，而 chunkserver 里的一个 chunk 的大小是 64MB。所以，在记录追加需要在chunk 里填空数据的时候，最多也就是填入 16MB，也就是 chunkserver 的存储空间最多会浪费 1/4。

**如果在主副本上写入成功了，但是在次副本上写入失败了怎么办呢？这样不是还会出现数据不一致的情况吗？**

![](https://s3.uuu.ovh/imgs/2023/04/03/a7f586b0a0db968c.png)

其实在这个时候，主副本会告诉客户端数据写入失败，然后让客户端重试。不过客户端发起的重试，并不是在原来的位置去写入数据，而是发起一个新的记录追加操作。这个时候，可能已经有其他的并发追加写入请求成功了，那么这次重试会写入到更后面。

我们可以一起来看这样一个例子：有三个客户端 X、Y、Z 并发向同一个文件进行记录追加，写入数据 A、B、C，对应的三个副本的 chunkserver 分别是 Q、P、R。

主副本先收到数据 A 的记录追加，在主副本和次副本上进行数据写入。在 A 写入的同时，B，C 的记录追加请求也来了，这个时候写入会并行进行，追加在 A 的后面。

这个时候，A 的写入在某个次副本 R 上失败了，于是主副本告诉客户端去重试；同时，客户端再次发起记录追加的重试，这次的数据写入，不在 A 原来的位置，而会是在 C 后面。

如此一来，在 B 和 C 的写入，以及 A 的重试完成之后，我们可以看到：

- 在 Q 和 P 上，chunkserver 里面的数据顺序是 A-B-C-A；
- 但是在 R 上，chunkserver 里面的数据顺序是 N/A-B-C-A；
- 也就是 Q 和 P 上，A 的数据被写入了两次，而在 R 上，数据里面有一段是有不可用的脏数据。

所以在这个记录追加的场景下，GFS 承诺的一致性，叫做`至少一次（At LeastOnce）`。也就是写入一份数据 A，在重试的情况下，至少会完整地在三个副本的同一个位置写入一次。但是也可能会因为失败，在某些副本里面写入多次。那么，在不断追加数据的情况下，你会看到大部分数据都是一致的，并且是确定的，但是整个文件中，会夹杂着少数不一致也不确定的数据。

而这个“至少一次”的写入模型也带来了两个巨大的好处。

第一是`高并发和高性能`，这个设计使得我们可以有很多个客户端并发向同一个 GFS 上的文件进行追加写入，而高性能本身也是我们需要分布式系统的起点。

第二是`简单`，GFS 采用了一个非常简单的单 master server，多个 chunkserver 架构，所有的协调动作都由 master 来做，而不需要复杂的一致性模型。毕竟，2003 年我们只有极其难以读懂的 Paxos 论文，Raft 这样的分布式共识算法要在 10 年之后的 2013 年才会诞生。而简单的架构设计，使得系统不容易出 Bug，出了各种 Bug 也更容易维护。

### <a name="chapter6"></a>总结

下图是基于 GFS 文件系统发展出来的文件系统分支：

![](https://s3.uuu.ovh/imgs/2023/04/03/67872208f279f44f.png)

### 参考资源

* [Spongecaptain's Blog](https://spongecaptain.cool/post/paper/googlefilesystem/#34-read-%E8%AF%BB%E6%93%8D%E4%BD%9C)

转载请注明：[sizheluo的博客](https://sizheluo.github.io) » [The Google File System论文](https://sizheluo.github.io/2023/03/The-Google-File-System%E8%AE%BA%E6%96%87/)